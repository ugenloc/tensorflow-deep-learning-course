{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_neural_network_classification_with_tensorflow_video.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOjjbY1fgxphSq8YqbPz1eo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Introduction to neural network classification with Tensorflow.\n","\n","In this notebook we are going to learn how to write neural network for classification problems.\n","\n","A classification is where you try to classify something as one thing or the other.\n","\n","A few types of classification problems includes:\n","\n","* Binary classification\n","* Multiclass classification\n","* Multilabel classification\n"],"metadata":{"id":"VqQinmhkJ7QK"}},{"cell_type":"markdown","source":["# Creating data to view and fit"],"metadata":{"id":"AYNsu78gExZh"}},{"cell_type":"code","source":["from sklearn.datasets import make_circles\n","\n","# Make 1000 examples\n","n_samples = 1000\n","\n","# Create circles\n","X, y = make_circles(n_samples,\n","                    noise=0.03,\n","                    random_state=42)\n"],"metadata":{"id":"VQlEn4_kLT2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out the features\n","X"],"metadata":{"id":"KVrVxSaeOnT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the labels\n","y[:10]"],"metadata":{"id":"SRahTHmROtt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Our data is a little hard to understand right now let visualize it "],"metadata":{"id":"Bv8VwIvbO6M1"}},{"cell_type":"code","source":["import pandas as pd\n","circles = pd.DataFrame({\"X0\":X[:,0], \"X1\":X[:, 1], \"label\":y}) \n","circles"],"metadata":{"id":"fgQx4R_qTZkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["circles[\"label\"].value_counts()"],"metadata":{"id":"TdQ-tiqxhq9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize with a plot \n","import matplotlib.pyplot as plt\n","plt.scatter(X[:,0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)"],"metadata":{"id":"vHGJlZV0T9iF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## IInput and output of our features and labels\n","X.shape, y.shape"],"metadata":{"id":"nVeHLnfeUiri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets check homany samples we are working with\n","len(X), len(y)"],"metadata":{"id":"pmkp7YCkWrJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View the first examples of features and labels \n","X[0], y[0]"],"metadata":{"id":"sN4InixZXBxx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Steps in modeling \n","\n","steps in modeling with tensorflow are typically:\n","\n","1. Create or import a model\n","2. Compile the model\n","3. Fit the model\n","4. Evaluate the model\n","5. Tweak the model\n","6. Evaluate..."],"metadata":{"id":"ccNvZlFxXUaZ"}},{"cell_type":"code","source":["import tensorflow as tf\n","# Set the random seed \n","tf.random.set_seed(42)\n","\n","# 1. Create the model using the sequential API\n","model_1 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(1)\n","])\n","\n","# 2. Compile the model\n","model_1.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","                optimizer=tf.keras.optimizers.SGD(),\n","                metrics=[\"accuracy\"])\n","\n","# 3. Fit the model\n","model_1.fit(tf.expand_dims(X, axis=-1), y, epochs=5)\n"],"metadata":{"id":"512UfEL6XpOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" #lets try to improve  our model by training for longer\n"," model_1.fit(tf.expand_dims(X, axis=-1), y, epochs=200, verbose=0)\n"," model_1.evaluate(X, y)"],"metadata":{"id":"GXyooof7bMTg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since we'er working on a binary classification problem and our model is getting around ~50% accuracy... it's perfoming as if it's guessing.\n","\n","So, let's step things up a notch and add some extra layaer"],"metadata":{"id":"-4uaB6KpdAT9"}},{"cell_type":"code","source":["# Set the random seed \n","tf.random.set_seed(42)\n","\n","# 1. Create a model, this time with 2 layers\n","model_2 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(1),\n","  tf.keras.layers.Dense(1)                             \n","])\n","\n","# 2, Compile the model\n","model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","                optimizer=tf.keras.optimizers.SGD(),\n","                metrics=[\"accuracy\"])\n","\n","# 3. fit the model\n","model_2.fit(tf.expand_dims(X, axis=-1), y, epochs=100, verbose=0)"],"metadata":{"id":"xhXuI-uzeSAW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 4. Evaluate the model\n","model_2.evaluate(X, y)"],"metadata":{"id":"lhpkEwkFgZZ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Improving our model\n","\n","Let's look into our bag of tricks to see how we can improve our model.\n","\n","1. Create a model - we might want to add more layers or increase the number of hidden units within a layer \n","2. Compiling a model - here we might wan to choose a different optimization function such as Adam instead of SGD.\n","3. Fitting a model - perhaps we might fit our model for more epochs(leave it training for longer). "],"metadata":{"id":"AB2kxw1Hglm6"}},{"cell_type":"code","source":["# Set the random seed\n","tf.random.set_seed(42)\n","\n","# 1.  Create the model (this time 3 layers)\n","model_3 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(100), # add 100 dense neurons\n","  tf.keras.layers.Dense(10),  # add another layer with 10 neurons\n","  tf.keras.layers.Dense(1)   \n","])\n","\n","# 2. Compile the model\n","model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])\n","\n","# 3. Fit the model\n","model_3.fit(tf.expand_dims(X, axis=-1), y, epochs=100)\n"],"metadata":{"id":"iST9VNNmpKEH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3.evaluate(X, y)"],"metadata":{"id":"yKrkl_b_9Kg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3.predict(X)"],"metadata":{"id":"_cizXIAA-Kq5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To visualize our model predictions, let's create a function plot_decision_boundary(), this function will:\n","\n","* Take in a trained model, feature(X) and label (y)\n","* Create a meshgrid of the different X value_counts\n","* Make predictions accross the meshgrid\n","* Plot the predictions as well as a line between zones (where each unique class falls) "],"metadata":{"id":"eDiK822Q_QPD"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"0QD37BlwDA7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_decision_boundary(model, X, y):\n","  \"\"\"\n","  Plots the decision boundary created by a model predicting on X.\n","  This function has been adapted from two phenomenal resources:\n","   1. CS231n - https://cs231n.github.io/neural-networks-case-study/\n","   2. Made with ML basics - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n","  \"\"\"\n","  # Define the axis boundaries of the plot and create a meshgrid\n","  x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n","  y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n","  xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n","                       np.linspace(y_min, y_max, 100))\n","  \n","  # Create X values (we're going to predict on all of these)\n","  x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together: https://numpy.org/devdocs/reference/generated/numpy.c_.html\n","  \n","  # Make predictions using the trained model\n","  y_pred = model.predict(x_in)\n","\n","  # Check for multi-class\n","  if model.output_shape[-1] > 1: # checks the final dimension of the model's output shape, if this is > (greater than) 1, it's multi-class \n","    print(\"doing multiclass classification...\")\n","    # We have to reshape our predictions to get them ready for plotting\n","    y_pred = np.argmax(y_pred, axis=1).reshape(xx.shape)\n","  else:\n","    print(\"doing binary classifcation...\")\n","    y_pred = np.round(np.max(y_pred, axis=1)).reshape(xx.shape)\n","  \n","  # Plot decision boundary\n","  plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n","  plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n","  plt.xlim(xx.min(), xx.max())\n","  plt.ylim(yy.min(), yy.max())\n"],"metadata":{"id":"6TKAxBniAmzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out the predictions our model is making\n","plot_decision_boundary(model=model_3,\n","                       X=X,\n","                       y=y)"],"metadata":{"id":"YJDTf1c_HiKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the variables of our plot functions\n","x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n","y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1"],"metadata":{"id":"J_uMuWrxCUNL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n","                       np.linspace(y_min, y_max, 100))"],"metadata":{"id":"iwvdqkI3Dld_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2.summary"],"metadata":{"id":"61FDZGxb7OGP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some code will not run due to the fact that there has been an upgrade to tf__version__ .\n","\n","the code below is the correct way to train neural network in tf version 2.7 and above."],"metadata":{"id":"_W-jxL4YWkAZ"}},{"cell_type":"code","source":["# Set random seed\n","tf.random.set_seed(42)\n","     \n","# 1. Create the model (this time 3 layers)\n","model_3 = tf.keras.Sequential([\n","  ## Before TensorFlow 2.7.0\n","  # tf.keras.layers.Dense(100), # add 100 dense neurons\n","     \n","  ## After TensorFlow 2.7.0\n","  tf.keras.layers.Dense(100, input_shape=(None, 1)), # <- define input_shape here\n","  tf.keras.layers.Dense(10), # add another layer with 10 neurons\n","  tf.keras.layers.Dense(1)\n","    ])\n","     \n","# 2. Compile the model\n","model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","                optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD\n","                metrics=['accuracy'])\n","\n"],"metadata":{"id":"-EI47g0zW_NI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set random seed\n","tf.random.set_seed(42)\n","     \n","# Create some regression data\n","X_regression = np.arange(0, 1000, 5)\n","y_regression = np.arange(100, 1100, 5)\n","     \n","# Split it into training and test sets\n","X_reg_train = X_regression[:150]\n","X_reg_test = X_regression[150:]\n","y_reg_train = y_regression[:150]\n","y_reg_test = y_regression[150:]\n","     \n","# Fit our model to the data\n","     \n","    ## Note: Before TensorFlow 2.7.0, this line would work\n","    # model_3.fit(X_reg_train, y_reg_train, epochs=100) # <- this will error in TensorFlow 2.7.0+\n","     \n","## After TensorFlow 2.7.0\n","model_3.fit(tf.expand_dims(X_reg_train, axis=-1), # <- expand input dimensions\n","            y_reg_train,\n","            epochs=100)"],"metadata":{"id":"1nQMc6WzXYNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set random seed\n","tf.random.set_seed(42)\n","\n","# 1. Create the model\n","model_4 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(100),\n","  tf.keras.layers.Dense(10),\n","  tf.keras.layers.Dense(1)                             \n","])\n","\n","# 2. Compile the model, this time with a regression-specific loss function\n","model_4.compile(loss=tf.keras.losses.mae,\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"mae\"])\n","\n","# 3. Fit the model\n","model_4.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs=100)"],"metadata":{"id":"cHGeWbbHXz62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions with our trained model\n","y_reg_preds = model_4.predict(X_reg_test)\n","\n","# Plot the model's predictions against our regression data\n","plt.figure(figsize=(10, 7))\n","plt.scatter(X_reg_train, y_reg_train, c=\"b\", label=\"Training data\")\n","plt.scatter(X_reg_test, y_reg_test, c=\"g\", label=\"Test data\")\n","plt.scatter(X_reg_test, y_reg_preds, c=\"r\", label=\"predictions\")\n","plt.legend();"],"metadata":{"id":"A9rDxfb9bf0w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The missing piece: Non-linearity is one of the most important concetps in neural network.\n","\\"],"metadata":{"id":"XuqpTrg-daof"}},{"cell_type":"code","source":["# Set randim seed\n","tf.random.set_seed(42)\n","\n","model_5 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(1, activation=tf.keras.activations.linear)                             \n","])\n","\n","# 2. Compile the model\n","model_5.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(lr=0.001),\n","                metrics=['accuracy'])\n","\n","# 3 Fit the model\n","history = model_5.fit(tf.expand_dims(X, axis=-1), y, epochs=100)\n"],"metadata":{"id":"N21XrVFHiYjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out our data\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu)"],"metadata":{"id":"JnQUcSi5HkjW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Check the decision boundary for our latest model\n","plot_decision_boundary(model=model_5,\n","                       X=X,\n","                       y=y)"],"metadata":{"id":"kkrcVjaMQniH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try build our first neural network with a non-linear activation function\n"],"metadata":{"id":"HG7R0TrTRiYL"}},{"cell_type":"code","source":["# Set random seed\n","tf.random.set_seed(42)\n","\n","# 1. Build the model\n","model_6 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(1, activation=tf.keras.activations.relu)                             \n","])\n","\n","# 2. Compile the model\n","model_6.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(lr=0.001),\n","                metrics=[\"accuracy\"])\n","\n","# 3. Fit the model\n","history = model_6.fit(tf.expand_dims(X, axis=-1,), y, epochs=100)"],"metadata":{"id":"q8iiRdChTMnX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Time to replicate the multi-layer neural network hypothesized on Tensorflow playground\n","\n","# Set the random seed\n","tf.random.set_seed(42)\n","\n","# Create the model\n","model_7 = tf.keras.Sequential([\n","   tf.keras.layers.Dense(4, activation=\"relu\"),\n","   tf.keras.layers.Dense(4, activation=\"relu\"),\n","   tf.keras.layers.Dense(1)                         \n","])\n","\n","# Compile the model\n","model_7.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(lr=0.001),\n","                metrics=[\"accuracy\"])\n","\n","# Fit the model\n","history = model_7.fit(tf.expand_dims(X, axis=-1,), y, epochs=100)"],"metadata":{"id":"UiQKcoQRVLw_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model\n","model_7.evaluate(X,y)"],"metadata":{"id":"NY0_nuZiZ4YK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How do our model_7 prediction look?\n","plot_decision_boundary(model_7, X, y)"],"metadata":{"id":"KSXh_f2na4RY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build model_8 with and output layers that has a sigmoid activation finction\n","# When solving binary classification problems, there is no need to expand the dimensions else you'll run into a shape error.\n","\n","# Set random seed\n","tf.random.set_seed(42)\n","\n","# Create a model\n","model_8 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 1, ReLU activation\n","  tf.keras.layers.Dense(4, activation=tf.keras.activations.relu), # hidden layer 2, ReLU activation\n","  tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid) # ouput layer, sigmoid activation\n","])\n","\n","# Compile the model\n","model_8.compile(loss=tf.keras.losses.binary_crossentropy,\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=['accuracy'])\n","\n","# Fit the model\n","history = model_8.fit(X, y, epochs=100, verbose=1)\n","\n"],"metadata":{"id":"N6by1GY3b2D4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"fxq9fPcihpQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_8.summary()"],"metadata":{"id":"CrVNuUa2XH4w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is wrong with our model? Why is it returning a TypeError saying 'NoneType' object is not callable. Are we really evaluating our model correctly? What data did the model learn on and what data did the model predict on?\n","\n","\n","**Note:** The combination of **linear (straight lines) and non-linear (non-straight lines) functions** is one of the key fundamentals of neural networks.\n","\n"],"metadata":{"id":"blC7ZW2QyWxE"}},{"cell_type":"code","source":["# Set random seed\n","tf.random.set_seed(42) # For reproduceability\n","\n","# 1. Create the model (this time 3 layers)\n","model_3 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(100), # add 100 dense neurons\n","  tf.keras.layers.Dense(10), # add another layer with 10 neurons\n","  tf.keras.layers.Dense(1)\n","])\n","\n","# 2. Compile the model\n","model_3.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n","                optimizer=tf.keras.optimizers.Adam(), # use Adam instead of SGD\n","                metrics=['accuracy'])\n","\n","## Create data\n","# Set random seed\n","tf.random.set_seed(42)\n","\n","# Create some regression data\n","X_regression = np.arange(0, 1000, 5)\n","y_regression = np.arange(100, 1100, 5)\n","\n","# Split it into training and test sets\n","X_reg_train = X_regression[:150]\n","X_reg_test = X_regression[150:]\n","y_reg_train = y_regression[:150]\n","y_reg_test = y_regression[150:]\n","\n","# Fit our model to the data\n","# -> Note: Before TensorFlow 2.7.0, this line would work <- \n","model_3.fit(tf.expand_dims(X_reg_train, axis=-1),y_reg_train, epochs=100)"],"metadata":{"id":"cuvmCICTfu_r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278"],"metadata":{"id":"OCsRYiAsOqj9"}},{"cell_type":"markdown","source":["The combination on of linear andf non linear functions is one of the key funbdamantals of neural networks."],"metadata":{"id":"q0PVY5ljlq99"}},{"cell_type":"code","source":["# Create a toy tensor (similar to the data we pass into the model)\n","A = tf.cast(tf.range(-10, 10), tf.float32)\n","A"],"metadata":{"id":"xHkI7_38l59j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(A)"],"metadata":{"id":"UjhzGWuGRP0v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sigmoid - https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid\n","\n","def sigmoid(x):\n","  return 1 / (1 + tf.exp(-x))\n","\n","# Use the sigmoid function in ouyr tensor\n","sigmoid(A)"],"metadata":{"id":"HrOeZfpdRyzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot sigmoid modified tensor\n","plt.plot(sigmoid(A))"],"metadata":{"id":"L7VUVERmSn4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Relu - https://www.tensorflow.org/apo_docs/python/tf/keras/activations/relu\n","\n","def relu(x):\n","  return tf.maximum(0, x)\n","\n","# Pass toy tensor through ReLu function\n","relu(A)"],"metadata":{"id":"8ggZOQB2S2AK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(relu(A))"],"metadata":{"id":"UElgjGx3Tesj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Linear - https://www.tensorflow.org/api_docs/python/tf/kerasactivations/linear (return inpot non-modified)\n","tf.keras.activations.linear(A)"],"metadata":{"id":"opmwFUSiTmk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["A == tf.keras.activations.linear(A)"],"metadata":{"id":"1CuIyHkeUG0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model dosen't learn anything when using linear activation finction"],"metadata":{"id":"oMzcWWzSUOQj"}},{"cell_type":"markdown","source":["# Evaluating and improving our classification model\n","\n","web been training and evaluating  our dataset on thye same sample. Now, let's split our dataset into training and test set "],"metadata":{"id":"UvyNiCnqcKOw"}},{"cell_type":"code","source":["# Firstly let's check the total num,be of training example we have \n","len(X)"],"metadata":{"id":"Jx_oNQyMcrKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split data into train and test set\n","X_train, y_train = X[:800], y[:800] # 80% of the data for thge training set\n","X_test, y_test = X[800:], y[800:] # 205 of the data for test set\n","\n","# Check the shape of the data \n","X_train.shape, X_test.shape # 300 examples in the training set, 200 examples in the test set "],"metadata":{"id":"QY6q-0CMdzYb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Excellent, we've split our dataset into train and test set. Now let's see how the model will oerform when evaluating on the test set"],"metadata":{"id":"nY2Sf7LJe1Nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a model with 2 hidden layers of 4 neurons each and a sigmoid output layer\n","tf.random.set_seed(42)\n","\n","# Create the model\n","model_9 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","# Compile the model\n","model_9.compile(loss=\"binary_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","                metrics=['accuracy'])\n","\n","# Fit the model\n","model_9.fit(X_train, y_train, epochs=25)"],"metadata":{"id":"A_-N7lAqfOjx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate our model on the test set\n","loss, accuracy = model_9.evaluate(X_test, y_test)\n","print(f\"Model loss on the test set: {loss}\")\n","print(f\"Model accuracy on the test set: {100*accuracy:.2f}%\")"],"metadata":{"id":"Cwogtp8QgyLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot decision boundary for the training and test set \n","plt.figure(figsize=(12,6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model_9, X=X_train, y=y_train)\n","plt.subplot(1, 2, 2)\n","plot_decision_boundary(model_9, X=X_train, y=y_train)\n","plt.show"],"metadata":{"id":"uAV3OEPRLFyM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The history variable that holds our model.fit() finction contains information on how our model learns"],"metadata":{"id":"KnitzYkaNvQ9"}},{"cell_type":"code","source":["# You can access the infomatio in the history variable using the .history attribute\n","pd.DataFrame(history.history)"],"metadata":{"id":"_5p1chDaMyjZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot the loss curve\n","pd.DataFrame(history.history).plot()\n","plt.title(\"Model_9 training curve\")"],"metadata":{"id":"dXZXEKDONjGO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The idea plot we are looking for when dealing with a classification problem is:\n","* Loss going down \n","* Accuracy going up\n","\n","When the loss decreases it means the model is improving (the predictions it is making is getting clossert to the grand truth label)"],"metadata":{"id":"BQLm9piuO3D_"}},{"cell_type":"markdown","source":["# Finding the best learning rate\n","\n","Aside from the architecture(the layers, number of neurons, activations, etc), the most important hyperparameter you can tune for your neural network models is the **learning rate**\n","\n","**Learning rate callback**\n","Think of a callback as an extra piece of functionality you can add to your m,odel while its is training.\n","\n","It's a  good pratice to try the default learning rate first before tweaking "],"metadata":{"id":"_d65XKCUO2wM"}},{"cell_type":"code","source":["# Set random seed\n","tf.random.set_seed(42)\n","\n","# Create the model \n","model_10 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","\n","# Compile the model \n","model_10.compile(loss=\"binary_crossentropy\",\n","                 optimizer=\"Adam\",\n","                 metrics=['accuracy'])\n","\n","# Create a learning rate scheduler callback\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n","\n","# Fit the model\n","history = model_10.fit(X_train,\n","                       y_train,\n","                       epochs=100,\n","                       callbacks=[lr_scheduler])"],"metadata":{"id":"Q3PC21AOOvpE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(history.history).plot(figsize=(10,7), xlabel=\"epochs\");"],"metadata":{"id":"gA5YMJjzWCXM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the learning rate versus the loss\n","lrs = 1e-4 * (10 ** (np.arange(100)/20))\n","plt.figure(figsize=(10, 7))\n","plt.semilogx(lrs, history.history[\"loss\"])\n","plt.xlabel(\"Learning Rate\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Learning rate vs loss\");"],"metadata":{"id":"WLhQCyl6YMAX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To figurer ou theidea value of the learning rate (at least the idea value to begin training our model), the rule of thumb is to take the learning rate value where the loss is still decreasing but not totally flattened out (usually about 10x smaller than the bottom of the curve).plot_decision_boundary}\n","The idea learning rate to start of model training is somewhere just before the loss curve bottoms out(a value where the loss is still decreasing)"],"metadata":{"id":"W4ZB4kx7bYg4"}},{"cell_type":"code","source":["# Examples of other typical learning rate values\n","10**0, 10**-1, 10**-2, 10**-3, 1e-4"],"metadata":{"id":"QzGqs8SydmRB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's create a new model with a learning rate of 0.02\n","\n","# Set random seed \n","tf.random.set_seed(42)\n","\n","# Create the model \n","model_11 = tf.keras.Sequential([\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(1, activation=\"sigmoid\")                                       \n","])\n","\n","# Compile the model\n","model_11.compile(loss=\"binary_crossentropy\",\n","                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.02),\n","                 metrics=[\"accuracy\"])\n","\n","# Fit the model\n","history = model_11.fit(X_train, \n","             y_train,\n","             epochs=20)"],"metadata":{"id":"bJoThcoseAyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the decision boundary for the training and test set \n","plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.title(\"Train\")\n","plot_decision_boundary(model_11, X=X_train, y=y_train)\n","plt.subplot(1, 2, 2)\n","plt.title(\"Test\")\n","plot_decision_boundary(model_11, X=X_test, y=y_test)\n","plt.show()"],"metadata":{"id":"mdYKOn8TjzY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's check the accuracy of our model\n","loss, accuracy = model_11.evaluate(X_test, y_test)\n","print(f\"model loss test set: {loss}\")\n","print(f\"model accuracy on test set: {(accuracy*100):.2f}%\")"],"metadata":{"id":"dfCigCsxk3Y-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can make a confussion matrix using a confussion matrix method\n","from sklearn.metrics import confusion_matrix\n","\n","# Make a prediction\n","y_preds = model_11.predict(X_test)\n","\n","# Create confussion matrix\n","confusion_matrix(y_test, y_preds)"],"metadata":{"id":"PPviIt2IQE4D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our predictions are not in the format they need to be\n"],"metadata":{"id":"kMZmkXCUReCa"}},{"cell_type":"code","source":["# Let's view the first 10 predictions\n","y_preds[:10]"],"metadata":{"id":"pXktt6qQSwrs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our y_pred is a prediction probability fomart. One of the set you will often see after making a prediction with a neural network is converting the prediction probability into labels.\n","\n"],"metadata":{"id":"ESRvuH8YTwrv"}},{"cell_type":"code","source":["# Lets's view the first 10 labels\n","y_test[:10]"],"metadata":{"id":"DX8PqGRjS9lP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the prediction probability into label using tf.round() amnd view the first 10 rows\n","tf.round(y_preds)[:10]"],"metadata":{"id":"jwRlw36TTREK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's re-create our confussion matrix\n","confusion_matrix(y_test, tf.round(y_preds))"],"metadata":{"id":"sKzIThcWUnxu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import itertools\n","\n","figsize = (10, 10)\n","\n","# Create the confussion matrix\n","cm = confusion_matrix(y_test, tf.round(y_preds))\n","cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n","n_classes = cm.shape[0]\n","\n","# Let's prettify it\n","fig, ax = plt.subplots(figsize=figsize)\n","# Create a matrix plot \n","cax = ax.matshow(cm, cmap=plt.cm.Blues)\n","fig.colorbar(cax)\n","\n","# Create classes\n","classes = False\n","\n","if classes:\n","  labels = classes\n","else:\n","  labels = np.arange(cm.shape[0])\n","\n","# Label the axes\n","ax.set(title=\"Confussion Matrix\",\n","       xlabel=\"Predicted label\",\n","       ylabel=\"True label\",\n","       xticks=np.arange(n_classes),\n","       yticks=np.arange(n_classes),\n","       xticklabels=labels,\n","       yticklabels=labels)\n","\n","# Set x-axis label to bottom\n","ax.xaxis.set_label_position(\"bottom\")\n","ax.xaxis.tick_bottom()\n","\n","# Adjust label size \n","ax.xaxis.label.set_size(20)\n","ax.yaxis.label.set_size(20)\n","ax.title.set_size(20)\n","\n","# Set threshold for different colors\n","threshold = (cm.max() + cm.min()) / 2\n","\n","# Plot the text on each cell\n","for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","  plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n","           horizontalalignment=\"center\",\n","           color=\"white\" if cm[i, j] > threshold else \"black\",\n","           size=15)"],"metadata":{"id":"0mVyhHveUyuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# What does intertools.product do? It combines two things into each combination\n","import itertools \n","for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","  print(1, j)"],"metadata":{"id":"JBVMvnuvXLN-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multiclass Classification\n","Multiclass classification is predit one out of many given examples.\n","\n","Everything we've learnt so far is applicable to multiclass classification.\n","\n","Let's import som data from Tensorflow dataset module (tf.keras.datasets)\n"],"metadata":{"id":"3W_tqLTqX41C"}},{"cell_type":"code","source":["import tensorflow as tf \n","from tensorflow.keras.datasets import fashion_mnist\n","\n","# The data has already been sorted into training and test set for us \n","(train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()"],"metadata":{"id":"1FWGe67SYX67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show the first training example\n","print(f\"Training sample:\\n{train_data[0]}\\n\")\n","print(f\"Training label: {train_labels[0]}\")"],"metadata":{"id":"ZfbEwlK2YX0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the shape of our data\n","train_data.shape, train_labels.shape, test_data.shape, test_data.shape"],"metadata":{"id":"DS_EDztiZmPf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot a single example\n","import matplotlib.pyplot as plt\n","plt.imshow(train_data[7])"],"metadata":{"id":"S3ys4fHFcsuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check our sample label\n","train_labels[7]"],"metadata":{"id":"o4UXi90Qi0LX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's create a small list of class name \n","class_name = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","              'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankel boot']\n","\n","# How many classes are there (this'll be our output shape)?\n","len(class_name)"],"metadata":{"id":"_C0PjGhCjGfh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Erro1"],"metadata":{"id":"W5mYGpKSry0N"}},{"cell_type":"code","source":["# Plot an example image and its label\n","plt.imshow(train_data[17], cmap=plt.cm.binary) # change the color to black and white\n","plt.title(class_name[train_labels[17]]) "],"metadata":{"id":"XQbBEPD8nBYT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Error 2"],"metadata":{"id":"yD64Xpf-sBxR"}},{"cell_type":"code","source":["# Plot multiple random images of fashion MNIST\n","import random\n","plt.figure(figsize=(7, 7))\n","for i in range(4):\n","  ax = plt.subplot(2, 2, i + 1)\n","  rand_index = random.choice(range(len(train_data)))\n","  plt.imshow(train_data[rand_index], cmap=plt.cm.binary)\n","  plt.title(class_name[train_labels[rand_index]])\n","  plt.axis(False)"],"metadata":{"id":"NBlJAtPln3lh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["let's build a model to figure out the relatonship between the pixel values and their labels"],"metadata":{"id":"y7NuvQM4pxci"}},{"cell_type":"code","source":["# Set random seed \n","tf.random.set_seed(42)\n","\n","# Create the model\n","model_14 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28,28)),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")\n","])\n","\n","# Compile the model\n","model_14.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","                 optimizer=tf.keras.optimizers.Adam(),\n","                 metrics=['accuracy'])\n","\n","# Fit the model\n","non_norm_history = model_14.fit(train_data,\n","                                train_labels,\n","                                epochs=10,\n","                                validation_data=(test_data, test_labels))"],"metadata":{"id":"ujudk2nIs1_k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_14.summary()"],"metadata":{"id":"7GFfvvoO0ouu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Our model did not perform well probably because we did not normalize our dataset "],"metadata":{"id":"ptsJ3V6c3meH"}},{"cell_type":"code","source":["# Let's check the min and max value of our dataset\n","train_data.min(), train_data.max()"],"metadata":{"id":"BxxQbdjS68I3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Scalling/ Normalizing our data.\n","\n","We can scale or noprmalize our data by simply dividing the entire array by the maximum"],"metadata":{"id":"3kewu7TB7KC9"}},{"cell_type":"code","source":["# Divide train and test images by maximum value (normalize it)\n","train_data = train_data / 255.0\n","test_data = test_data / 255.0\n","\n","# Check the min and max values of the training data\n","train_data.min(), train_data.max()"],"metadata":{"id":"mSDleJxG8W41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now, let us retrain our data wit the normalized train and test data\n","\n","# Set random seed \n","tf.random.set_seed(42)\n","\n","# Create the model\n","model_15 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28,28)),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")                                \n","])\n","\n","# Compile the model\n","model_15.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","                 optimizer=tf.keras.optimizers.Adam(),\n","                 metrics=['accuracy'])\n","\n","# Fit the model (to the normalized data)\n","norm_history = model_15.fit(train_data,\n","                                train_labels,\n","                                epochs=10,\n","                                validation_data=(test_data, test_labels))"],"metadata":{"id":"ew7lgdGI886Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When comparing models make sure you aree cpmparing them on the same criterias (e.g same archetecture but different data or same data but different architecture)"],"metadata":{"id":"s8KtDDtGJ38F"}},{"cell_type":"code","source":["# Lets plut each model history with loss curves\n","# non-normalized data loss curve\n","import pandas as pd\n","pd.DataFrame(non_norm_history.history).plot(title=\"Non-normalized Data\")\n","# Plot normalized data loss curves\n","pd.DataFrame(norm_history.history).plot(title=\"Normalized data\");"],"metadata":{"id":"34eTueNTAHQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set random seed \n","tf.random.set_seed(42)\n","\n","# Create the model\n","model_16 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")                                \n","])\n","\n","# Compile the model\n","model_16.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","                 optimizer=tf.keras.optimizers.Adam(),\n","                 metrics=[\"accuracy\"])\n","\n","# Create the learning rate callback\n","lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20))\n","\n","# Fit the model\n","find_lr_history = model_16.fit(train_data,\n","                               train_labels,\n","                               epochs=40,\n","                               validation_data=(test_data, test_labels),\n","                               callbacks=[lr_scheduler])\n"],"metadata":{"id":"ewO4Mc2DGVfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot the learning rate decay curve\n","import numpy as np\n","import matplotlib.pyplot as plt\n","lrs = 1e-3 * (10**(np.arange(40)/20))\n","plt.semilogx(lrs, find_lr_history.history[\"loss\"])\n","plt.xlabel(\"Learning Rate\")\n","plt.ylabel(\"loss\")\n","plt.title(\"Finding the ideal learning rate\");"],"metadata":{"id":"2-5lQYD9Twje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["10**-2\n"],"metadata":{"id":"bF3MpbMHXhdt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets refit our model using the ideal learning rate\n","tf.random.set_seed(42)\n","\n","# Create the model\n","model_17 = tf.keras.Sequential([\n","  tf.keras.layers.Flatten(input_shape=(28, 28)),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(4, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"softmax\")                               \n","])\n","\n","# Compile the model\n","model_17.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","                 optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n","                 metrics=[\"accuracy\"])\n","\n","# Fit the model\n","history = model_14.fit(train_data,\n","                       train_labels,\n","                       epochs=20,\n","                       validation_data=(test_data, test_labels))"],"metadata":{"id":"r6Fbbts3XnMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Note: The following confusion matrix code is a remix of Scikit-Learn's \n","# plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n","# and Made with ML's introductory notebook - https://github.com/GokuMohandas/MadeWithML/blob/main/notebooks/08_Neural_Networks.ipynb\n","import itertools\n","from sklearn.metrics import confusion_matrix\n","\n","# Our function needs a different name to sklearn's plot_confusion_matrix\n","def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15): \n","  \"\"\"Makes a labelled confusion matrix comparing predictions and ground truth labels.\n","\n","  If classes is passed, confusion matrix will be labelled, if not, integer class values\n","  will be used.\n","\n","  Args:\n","    y_true: Array of truth labels (must be same shape as y_pred).\n","    y_pred: Array of predicted labels (must be same shape as y_true).\n","    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.\n","    figsize: Size of output figure (default=(10, 10)).\n","    text_size: Size of output figure text (default=15).\n","  \n","  Returns:\n","    A labelled confusion matrix plot comparing y_true and y_pred.\n","\n","  Example usage:\n","    make_confusion_matrix(y_true=test_labels, # ground truth test labels\n","                          y_pred=y_preds, # predicted labels\n","                          classes=class_names, # array of class label names\n","                          figsize=(15, 15),\n","                          text_size=10)\n","  \"\"\"  \n","  # Create the confustion matrix\n","  cm = confusion_matrix(y_true, y_pred)\n","  cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis] # normalize it\n","  n_classes = cm.shape[0] # find the number of classes we're dealing with\n","\n","  # Plot the figure and make it pretty\n","  fig, ax = plt.subplots(figsize=figsize)\n","  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better\n","  fig.colorbar(cax)\n","\n","  # Are there a list of classes?\n","  if classes:\n","    labels = classes\n","  else:\n","    labels = np.arange(cm.shape[0])\n","  \n","  # Label the axes\n","  ax.set(title=\"Confusion Matrix\",\n","         xlabel=\"Predicted label\",\n","         ylabel=\"True label\",\n","         xticks=np.arange(n_classes), # create enough axis slots for each class\n","         yticks=np.arange(n_classes), \n","         xticklabels=labels, # axes will labeled with class names (if they exist) or ints\n","         yticklabels=labels)\n","  \n","  # Make x-axis labels appear on bottom\n","  ax.xaxis.set_label_position(\"bottom\")\n","  ax.xaxis.tick_bottom()\n","\n","  # Set the threshold for different colors\n","  threshold = (cm.max() + cm.min()) / 2.\n","\n","  # Plot the text on each cell\n","  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","    plt.text(j, i, f\"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)\",\n","             horizontalalignment=\"center\",\n","             color=\"white\" if cm[i, j] > threshold else \"black\",\n","             size=text_size)\n"],"metadata":{"id":"uB-epz6-e77L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make predictions with the most recent model\n","y_probs = model_17.predict(test_data)\n","\n","# View the first 5 predictions\n","y_probs[:5]"],"metadata":{"id":"rJ07a-5Aik47"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# See the predicted class number and label for the first example\n","y_probs[0].argmax(), class_name[y_probs[0].argmax()]"],"metadata":{"id":"aRiez8Z0jE0S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert all of the predictions from probabilities to labels\n","y_preds = y_probs.argmax(axis=1)\n","\n","# View the first 10 prediction labels\n","y_preds[:10]"],"metadata":{"id":"Xh61QE0Hj-gm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out the non-prettified confussion matrix\n","from sklearn.metrics import confusion_matrix\n","confusion_matrix(y_true=test_labels,\n","                  y_pred=y_preds)"],"metadata":{"id":"1ppsbZHHk87B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Make a prettier confusion matrix\n","make_confusion_matrix(y_true=test_labels, \n","                      y_pred=y_preds,\n","                      classes=class_name,\n","                      figsize=(15, 15),\n","                      text_size=10)\n"],"metadata":{"id":"WHOIwRoQmKQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","# Create a function for plotting a random image along with its prediction\n","def plot_random_image(model, images, true_labels, classes):\n","  \"\"\"Picks a random image, plots it and labels it with a predicted and truth label.\n","\n","  Args:\n","    model: a trained model (trained on data similar to what's in images).\n","    images: a set of random images (in tensor form).\n","    true_labels: array of ground truth labels for images.\n","    classes: array of class names for images.\n","  \n","  Returns:\n","    A plot of a random image from `images` with a predicted class label from `model`\n","    as well as the truth class label from `true_labels`.\n","  \"\"\" \n","  # Setup random integer\n","  i = random.randint(0, len(images))\n","  \n","  # Create predictions and targets\n","  target_image = images[i]\n","  pred_probs = model.predict(target_image.reshape(1, 28, 28)) # have to reshape to get into right size for model\n","  pred_label = classes[pred_probs.argmax()]\n","  true_label = classes[true_labels[i]]\n","\n","  # Plot the target image\n","  plt.imshow(target_image, cmap=plt.cm.binary)\n","\n","  # Change the color of the titles depending on if the prediction is right or wrong\n","  if pred_label == true_label:\n","    color = \"green\"\n","  else:\n","    color = \"red\"\n","\n","  # Add xlabel information (prediction/true label)\n","  plt.xlabel(\"Pred: {} {:2.0f}% (True: {})\".format(pred_label,\n","                                                   100*tf.reduce_max(pred_probs),\n","                                                   true_label),\n","             color=color) # set the color to green or red"],"metadata":{"id":"zLYZA1qymyVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check out a random image as well as its prediction\n","plot_random_image(model=model_14, \n","                  images=test_data, \n","                  true_labels=test_labels, \n","                  classes=class_name)"],"metadata":{"id":"pSQDd7kxoOal"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Find layers our of our most recent model\n","model_17.layers"],"metadata":{"id":"jkvNJKXKoi3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can extract a particular by indexing\n","model_17.layers[1]"],"metadata":{"id":"4tfNKe8Ipczm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Get the patterns of a layer in our network\n","weights, biases = model_14.layers[1].get_weights()\n","\n","# Shape = 1 weight matrix the size of our input data (28x28) per neuron (4)\n","weights, weights.shape\n"],"metadata":{"id":"buhBkQdYppsT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_17.summary(\"\")"],"metadata":{"id":"_16YaMqLqWfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import plot_model\n","\n","# See the inputs and outputs of each layer\n","plot_model(model_17, show_shapes=True)"],"metadata":{"id":"1HHfjABPrhUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"enqHFrnXuGYw"},"execution_count":null,"outputs":[]}]}